{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "####################################\n",
    "# 1) Load Data (Parquet Files)\n",
    "####################################\n",
    "# Must Use Official Dataset\n",
    "train_file = r\"C:\\Users\\taejo\\Desktop\\CSI-5388-project\\train_data.parquet\"\n",
    "test_file  = r\"C:\\Users\\taejo\\Desktop\\CSI-5388-project\\test_data.parquet\"\n",
    "\n",
    "df_train = pd.read_parquet(train_file)\n",
    "df_test  = pd.read_parquet(test_file)\n",
    "\n",
    "# Common preprocessing function\n",
    "def common_preprocessing(df):\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    # Create UID: combine Source IP and Destination IP\n",
    "    df['uid'] = df['Source IP'].str.strip() + \"-\" + df['Destination IP'].str.strip()\n",
    "    # Convert the Timestamp column to datetime and sort\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    df.sort_values('Timestamp', inplace=True)\n",
    "    # Clean up the Label column: remove whitespace and convert to uppercase (e.g., \"BENIGN\", \"DrDoS_DNS\", etc.)\n",
    "    df['Label'] = df['Label'].astype(str).str.strip().str.upper()\n",
    "    return df\n",
    "\n",
    "df_train = common_preprocessing(df_train)\n",
    "df_test  = common_preprocessing(df_test)\n",
    "\n",
    "print(\"Train dataset size:\", df_train.shape)\n",
    "print(\"Test dataset size:\", df_test.shape)\n",
    "\n",
    "####################################\n",
    "# 2) Set Parameters (Refer to Table 1 in the paper)\n",
    "####################################\n",
    "time_interval = '2min'      # Time interval T (e.g., 2 minutes)\n",
    "beta_lower    = 0.2         # Lower threshold for system entropy (βlower)\n",
    "delta_susp    = 0.15        # Entropy change threshold for suspicious cluster (δsusp)\n",
    "delta_attack  = 0.10        # Entropy change threshold for attacker cluster (δattack)\n",
    "\n",
    "####################################\n",
    "# 3) Entropy Calculation Function (Shannon Entropy)\n",
    "####################################\n",
    "def shannon_entropy(freqs):\n",
    "    total = np.sum(freqs)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    p = freqs / total\n",
    "    return -np.sum(p * np.log2(p + 1e-10))\n",
    "\n",
    "####################################\n",
    "# 4) Data Processing Function (Two-stage Attacker Detection by Time Interval, Multiclass Classification)\n",
    "####################################\n",
    "def process_data(df):\n",
    "    predictions = []\n",
    "    \n",
    "    # Group by time interval using pd.Grouper\n",
    "    for window_start, window_data in df.groupby(pd.Grouper(key='Timestamp', freq=time_interval)):\n",
    "        window_data = window_data.copy()\n",
    "        if len(window_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Step 1: Count requests per uid\n",
    "        user_counts = window_data.groupby('uid').size()\n",
    "        H_system = shannon_entropy(user_counts.values)\n",
    "        \n",
    "        # Initially set all predicted labels to \"BENIGN\" for all data\n",
    "        window_data['predicted_label'] = \"BENIGN\"\n",
    "        \n",
    "        # Step 2: If the overall system entropy is below βlower, mark the time window as suspicious\n",
    "        if H_system < beta_lower:\n",
    "            # Suspicious time window → Perform KMeans clustering for each uid (number of clusters: 3)\n",
    "            uid_df = pd.DataFrame({\n",
    "                'uid': user_counts.index,\n",
    "                'count': user_counts.values\n",
    "            })\n",
    "            uid_df['count_log'] = np.log1p(uid_df['count'])\n",
    "            \n",
    "            # If there are fewer than 3 active UIDs, clustering is not feasible → Keep the window as \"BENIGN\"\n",
    "            if uid_df.shape[0] < 3:\n",
    "                predictions.append(window_data)\n",
    "                continue\n",
    "            \n",
    "            scaler_uid = StandardScaler()\n",
    "            X_uid = scaler_uid.fit_transform(uid_df[['count', 'count_log']])\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "            uid_df['cluster'] = kmeans.fit_predict(X_uid)\n",
    "            \n",
    "            # Determine the order of clusters based on the mean count of each cluster\n",
    "            cluster_means = uid_df.groupby('cluster')['count'].mean().sort_values()\n",
    "            cluster_order = cluster_means.index.tolist()\n",
    "            # Mapping: low count → normal, medium → suspicious, high → attackers\n",
    "            cluster_label_map = {\n",
    "                cluster_order[0]: 'BENIGN',         # Normal\n",
    "                cluster_order[1]: 'SUSPICIOUS',       # Suspicious\n",
    "                cluster_order[2]: 'ATTACKERS'         # Attackers\n",
    "            }\n",
    "            uid_df['cluster_label'] = uid_df['cluster'].map(cluster_label_map)\n",
    "            \n",
    "            # Step 3: Calculate H_normal from the set of UIDs in the normal cluster\n",
    "            normal_uids = uid_df[uid_df['cluster_label'] == 'BENIGN']['uid']\n",
    "            normal_counts = user_counts[normal_uids] if len(normal_uids) > 0 else None\n",
    "            H_normal = shannon_entropy(normal_counts.values) if (normal_counts is not None and normal_counts.sum() > 0) else 0\n",
    "            \n",
    "            # Step 4: Additional entropy ratio check for suspicious and attacker clusters\n",
    "            def ratio_entropy(uid_candidate):\n",
    "                if normal_counts is None or normal_counts.sum() == 0:\n",
    "                    return 1.0\n",
    "                if uid_candidate in normal_counts.index:\n",
    "                    return 1.0\n",
    "                combined = pd.concat([normal_counts, pd.Series([user_counts.loc[uid_candidate]], index=[uid_candidate])])\n",
    "                return shannon_entropy(combined.values) / (H_normal + 1e-10)\n",
    "            \n",
    "            # Set of UIDs ultimately determined as attackers\n",
    "            final_attacker_uids = set()\n",
    "            # Apply threshold for each of the suspicious and attacker clusters\n",
    "            for row in uid_df.itertuples():\n",
    "                uid_val = row.uid\n",
    "                cl_label = row.cluster_label\n",
    "                if cl_label == 'BENIGN':\n",
    "                    continue\n",
    "                r = ratio_entropy(uid_val)\n",
    "                if cl_label == 'SUSPICIOUS' and r < delta_susp:\n",
    "                    final_attacker_uids.add(uid_val)\n",
    "                elif cl_label == 'ATTACKERS' and r < delta_attack:\n",
    "                    final_attacker_uids.add(uid_val)\n",
    "            \n",
    "            # Final prediction: For UIDs determined as attackers within the window, predict the label as the majority (mode) true label of that uid,\n",
    "            # otherwise predict \"BENIGN\".\n",
    "            # Get the mode of the true Label for each uid\n",
    "            uid_label_mode = window_data.groupby('uid')['Label'].agg(lambda x: x.mode().iloc[0])\n",
    "            \n",
    "            def assign_label(uid):\n",
    "                if uid in final_attacker_uids:\n",
    "                    return uid_label_mode.loc[uid]\n",
    "                else:\n",
    "                    return \"BENIGN\"\n",
    "            \n",
    "            window_data['predicted_label'] = window_data['uid'].apply(assign_label)\n",
    "        \n",
    "        # If the system entropy is high enough, the predictions in window_data remain \"BENIGN\".\n",
    "        predictions.append(window_data)\n",
    "    \n",
    "    return pd.concat(predictions).sort_index()\n",
    "\n",
    "####################################\n",
    "# 5) Process and Evaluate the Training Dataset (Multiclass)\n",
    "####################################\n",
    "df_train_pred = process_data(df_train)\n",
    "\n",
    "# Multiclass evaluation: use the original Label column as the true label\n",
    "cm_train = confusion_matrix(df_train_pred['Label'], df_train_pred['predicted_label'])\n",
    "accuracy_train = accuracy_score(df_train_pred['Label'], df_train_pred['predicted_label'])\n",
    "report_train = classification_report(df_train_pred['Label'], df_train_pred['predicted_label'])\n",
    "\n",
    "print(\"\\n--- Train Dataset Results ---\")\n",
    "print(\"Confusion Matrix (Rows: True, Columns: Predicted):\")\n",
    "print(cm_train)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_train * 100))\n",
    "print(report_train)\n",
    "\n",
    "####################################\n",
    "# 6) Process and Evaluate the Test Dataset (Multiclass)\n",
    "####################################\n",
    "df_test_pred = process_data(df_test)\n",
    "cm_test = confusion_matrix(df_test_pred['Label'], df_test_pred['predicted_label'])\n",
    "accuracy_test = accuracy_score(df_test_pred['Label'], df_test_pred['predicted_label'])\n",
    "report_test = classification_report(df_test_pred['Label'], df_test_pred['predicted_label'])\n",
    "\n",
    "print(\"\\n--- Test Dataset Results ---\")\n",
    "print(\"Confusion Matrix (Rows: True, Columns: Predicted):\")\n",
    "print(cm_test)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_test * 100))\n",
    "print(report_test)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assume that the confusion matrix (cm_test) has already been computed.\n",
    "# For example: cm_test = confusion_matrix(df_test_pred['Label'], df_test_pred['predicted_label'])\n",
    "\n",
    "# Set class names (e.g., \"Benign\", \"Attack\")\n",
    "class_names = df_test['Label'].unique()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(cm_test, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(\"Confusion Matrix (Test Dataset)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# If you wish to save the image to a file, use the following:\n",
    "plt.savefig(\"confusion_matrix_test.png\")\n",
    "\n",
    "# Normalize each row (true label): divide each cell by the row sum\n",
    "cm_norm = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(\"Normalized Confusion Matrix (Test Dataset)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"confusion_matrix_test_normalized.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
